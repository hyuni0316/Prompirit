![header](https://capsule-render.vercel.app/api?type=waving&height=250&section=header&text=ğŸª„Prompirit&fontAlignY=40&animation=fadeIn&fontColor=FFFFFF)

# ğŸª„Prompirit: Prompt Editing AI Tool to Create AI-Generative Work Using User Emotion

â€œì´ë¯¸ì§€ GenAI í”„ë¡¬í”„íŠ¸ ìµœì í™”ë¥¼ ì œê³µí•˜ëŠ” Prompiritì€ ì‚¬ìš©ì ë°œí™” ë° ì‚¬ìš©ì ê°ì •ì˜ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•˜ëŠ” Multi-modal AI í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ 
text promptë¥¼ ìƒì„±í•œ í›„, ì´ë¥¼ GenAIì˜ input í”„ë¡¬í”„íŠ¸ë¡œ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ê°ì •ì„ ë³´ë‹¤ ì •êµí•˜ê²Œ ë°˜ì˜í•œ ì´ë¯¸ì§€ ê²°ê³¼ë¬¼ì„ ìƒì„±í•˜ë„ë¡ í•œë‹¤.â€
<br/>
<br/>

## ğŸ’š Info
### 23-2, 24-1 ì´í™”ì—¬ìëŒ€í•™êµ ìº¡ìŠ¤í†¤ë””ìì¸ 
**18 ì•¤íŠ¸(AnT)** | @gamddalki @hyuni0316 @syou-b
<br/>
<br/>

## ğŸ’â€â™€ï¸ ëª©ì°¨
- [ì œì•ˆë‚´ìš©](#ì œì•ˆë‚´ìš©)
- [Research Questions](#research-questions)
- [êµ¬í˜„ë°©ë²•](#êµ¬í˜„ë°©ë²•)
- [ì„¸ë¶€ê¸°ìˆ ](#ì„¸ë¶€ê¸°ìˆ )
- [Contribution](#contribution)
- [References](#references)

<br/>

 ## ì œì•ˆë‚´ìš©
 Expressing emotions is a crucial behavior that benefits humans in various ways, such as enriching self-expression, self-awareness, and creativity. Recently, a number of generative AI models have been steadily mentioned to support this process by creating images through short text prompts. However, it is still uncertain whether generated images show user emotions in text prompts properly. In this paper, we propose a multi-modal prompt engineering AI tool, Prompirit, that improves AI-generated images by using userâ€™s emotions as a core feature. First we will curate a rubric to apply emotions in text prompts more precisely with automatic prompt engineering. With our rubric, we will make an attempt to improve the output image by adding voice and face data for emotion recognition. After that, we will conduct user study to confirm the effectiveness of Prompirit.
<br/>
<br/>
ê°ì • í‘œí˜„ì€ ìê¸°í‘œí˜„, ìê¸°ì¸ì‹, ì°½ì˜ì„±ì„ í’ë¶€í•˜ê²Œ í•˜ëŠ” ë“± ì—¬ëŸ¬ ë°©ë©´ì—ì„œ ì¸ê°„ì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ì¤‘ìš”í•œ í–‰ë™ì…ë‹ˆë‹¤. ìµœê·¼ ìƒì„±í˜• AIëŠ” ì´ëŸ¬í•œ ê³¼ì •ì„ ì§€ì›í•˜ëŠ” ë„êµ¬ë¡œì¨ ê¾¸ì¤€íˆ ì–¸ê¸‰ë˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìƒì„±ëœ ì´ë¯¸ì§€ê°€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë‚´ ì‚¬ìš©ìì˜ ê°ì •ì„ ëª…í™•íˆ ë°˜ì˜í•˜ëŠ”ì§€ëŠ” ì—¬ì „íˆ ë¶ˆí™•ì‹¤í•©ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì‚¬ìš©ìì˜ ê°ì •ì„ í•µì‹¬ ìš”ì†Œë¡œ ê³ ë ¤í•˜ì—¬ AI ìƒì„± ì´ë¯¸ì§€ë¥¼ ê°œì„ í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ AI ë„êµ¬ì¸ Prompiritì„ ì œì•ˆí•©ë‹ˆë‹¤. ë¨¼ì € ìë™í™”ëœ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ í†µí•´ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì—ì„œ ê°ì •ì„ ë³´ë‹¤ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ë„ë¡ í•˜ëŠ” ë£¨ë¸Œë¦­ì„ ë§Œë“¤ ê²ƒì…ë‹ˆë‹¤. ë˜í•œ, ì¶œë ¥ ì´ë¯¸ì§€ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´, ìŒì„± ë° ì–¼êµ´ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì—¬ ê°ì • ì¸ì‹ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ì‹œë„ë¥¼ í•  ê²ƒì…ë‹ˆë‹¤. ì´í›„ Prompiritì˜ íš¨ê³¼ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ì‚¬ìš©ì ì—°êµ¬ë¥¼ ìˆ˜í–‰í•  ê²ƒì…ë‹ˆë‹¤.
<br/>
<br/>

## Research Questions
**RQ1.** How can emotions in text prompts be applied to generated image more precisely with automatic engineering? <br/>
**RQ2.** How can emotion and information requests of image generative AI users be observed more precisely? <br/>
**RQ3.** How effective is Prompirit in generating images with emotional expression? <br/>
<br/>

## êµ¬í˜„ë°©ë²•
**RQ1. ì–´ë–»ê²Œ í•˜ë©´ ìë™í™”ëœ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ í†µí•´ í…ìŠ¤íŠ¸ ë‚´ì˜ ê°ì •ì„ ë”ìš± ì •êµí•˜ê²Œ ì´ë¯¸ì§€ ê²°ê³¼ë¬¼ì— ë°˜ì˜í•  ìˆ˜ ìˆëŠ”ê°€?**
- ì„ í–‰ì—°êµ¬ì¸ RePromptì—ì„œ ì‚¬ìš©ëœ processì¸ ë‹¨ì–´ ë‹¨ìœ„ í…ìŠ¤íŠ¸ ë¶„ì„ì„ ë¬¸ì¥ ë‹¨ìœ„ í…ìŠ¤íŠ¸ ë¶„ì„ê³¼ ë¹„êµ
- ë¶„ì„í•œ í…ìŠ¤íŠ¸ë¥¼ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ í˜•íƒœë¡œ ê°€ê³µ
- CLIP score ê³„ì‚°ì„ í†µí•´ íš¨ê³¼ì ì´ì—ˆë˜ ë°©ì‹ì„ rubricìœ¼ë¡œ ì±„íƒ
<br/>

**RQ2. ì–´ë–»ê²Œ í•˜ë©´ ì´ë¯¸ì§€ ìƒì„± AI ì‚¬ìš©ìì˜ ê°ì • ë° ì •ë³´ ìš”ì²­ì„ ë”ìš± ëª…í™•íˆ íŒŒì•…í•  ìˆ˜ ìˆëŠ”ê°€?**
- multi-modal (bi-modal, tri-modal) ë°©ì‹ì„ ë„ì…í•¨
- uni-modal vs multi-modal (bi-modal, tri-modal) ì—ì„œ ì¸ì‹ëœ ê°ì • ë° í…ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ê°€ê³µ í›„ ì´ë¯¸ì§€ ìƒì„±
- CLIP scoreë¥¼ ê³„ì‚°í•˜ì—¬ ê°ê´€ì ì¸ ì¸¡ì • ì§€í‘œë¡œ ì‚¬ìš©í•´ ë¹„êµ
<br/>

**RQ3. Prompiritì´ ê°ì •ì„ ë°˜ì˜í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ìƒì„±í•¨ì— ìˆì–´ì„œ ì–¼ë§ˆë‚˜ ë” íš¨ê³¼ì ì¸ê°€?**
- ìœ ì € ìŠ¤í„°ë””: ì‚¬ìš©ìì˜ ì–¼êµ´, ìŒì„±, í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•˜ì—¬ RQ1&RQ2ì˜ ë°©ì‹ìœ¼ë¡œ ì¡°í•©í•´ ê°ê° ì´ë¯¸ì§€ ìƒì„± â†’ ì–´ë–¤ ì´ë¯¸ì§€ê°€ ê°ì •ì„ ê°€ì¥ ì˜ ë°˜ì˜í•˜ì˜€ëŠ”ê°€ ì„ íƒ

<br/>

## ì„¸ë¶€ê¸°ìˆ 
<img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=Python&logoColor=white"> <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=PyTorch&logoColor=white"> <img src="https://img.shields.io/badge/OpenCV-5C3EE8?style=for-the-badge&logo=OpenCV&logoColor=white"> <img src="https://img.shields.io/badge/OpenAI-412991?style=for-the-badge&logo=OpenAI&logoColor=white"> <img src="https://img.shields.io/badge/firebase-FFCA28?style=for-the-badge&logo=firebase&logoColor=white"> 
</br>

- Generative AI - Stable diffusion / DALL.E
- Multi-modal Emotion Recognition - Tensor Fusion Network
- Training Dataset (multi modal) - ì˜ì–´(CMU-MOSI)/í•œêµ­ì–´(AI-Hub ë©€í‹°ëª¨ë‹¬ ì˜ìƒ)
- Multi-class sentiment analysis - KoBERT <br/>
<br/>

## Contribution
1. í…ìŠ¤íŠ¸ì˜ ê°ì • ë§¥ë½ì„ ì •êµí•˜ê²Œ ì½ì„ ìˆ˜ ìˆëŠ” ë©€í‹°ëª¨ë‹¬ AIë¥¼ í†µí•´ ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ ê°ì • ë§¥ë½ì— ì í•©í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŒ
2. human-readable prompt revisions: ì‚¬ìš©ìê°€ ëª¨ë¸ì„ ë” ì˜ ì´í•´í•˜ê³  trial-and-error ì‹œë„ íšŸìˆ˜ë¥¼ ì¤„ì´ë„ë¡ í•¨
3. ì •ì‹ ì˜í•™(ì‹¬ë¦¬ ìƒë‹´, ìŠ¤íŠ¸ë ˆìŠ¤ ì¡°ì ˆ, expressive writing ë“±)ì— í™œìš©ë  ìˆ˜ ìˆìŒ
<br/>

## References
[1] Yunlong Wang, Shuyuan Shen, and Brian Y. Lim. 2023. RePrompt: Automatic Prompt Editing to Refine AI-Generative Art Towards Precise Expressions. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI â€™23), April 23â€“28, 2023, Hamburg, Germany. ACM, New York, NY, USA, 29 pages. https://doi.org/10.1145/3544548.3581402
<br/>
[2] Jonas Oppenlaender. 2022. Prompt Engineering for Text-Based Generative Art. arXiv 1, 1. Retrieved from http://arxiv.org/abs/2204.13988
<br/>
[3] Ankita Gandhi, Kinjal Adhvaryu, Soujanya Poria, Erik Cambria, Amir Hussain, Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions, Information Fusion, Volume 91, 2023, Pages 424-444, ISSN 1566-2535, https://doi.org/10.1016/j.inffus.2022.09.025.
<br/>
[4] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2017.Â Tensor Fusion Network for Multimodal Sentiment Analysis. InÂ Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1103â€“1114, Copenhagen, Denmark. Association for Computational Linguistics.
<br/>
[5] Andrea Scarantino. 2017. How to Do Things with Emotional Expressions: The Theory of Affective Pragmatics. Psychological Inquiry 28, 2â€“3: 165â€“185. https://doi.org/10.1080/1047840X.2017.1328951
